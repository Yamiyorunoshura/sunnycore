name: Promptfoo CI Integration

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  promptfoo-evaluation:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install promptfoo PyYAML

    - name: Install Node.js (for promptfoo)
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    - name: Install promptfoo
      run: npm install -g promptfoo

    - name: Run behavior tests with promptfoo
      run: |
        python -c "
        import sys
        sys.path.append('src')
        from behavior.golden_set_manager import GoldenSetManager
        from behavior.promptfoo_integration import PromptfooIntegration

        # Initialize managers
        golden_manager = GoldenSetManager('tests/test_golden_set.json')
        promptfoo_integration = PromptfooIntegration()

        # Load test cases from golden set
        test_cases = golden_manager.get_all_test_cases()

        # Add test cases to promptfoo
        success_count = promptfoo_integration.add_multiple_test_cases(test_cases)
        print(f'Added {success_count} test cases to promptfoo')
        "

    - name: Run promptfoo evaluation
      run: |
        promptfoo eval --config promptfooconfig.yaml --output results.json --share

    - name: Upload evaluation results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: promptfoo-results
        path: results.json

    - name: Generate quality report
      run: |
        python -c "
        import sys
        sys.path.append('src')
        from behavior.metrics_calculator import QualityMetricsCalculator
        from behavior.llm_judge import LLMJudge
        import json

        # Load promptfoo results
        with open('results.json', 'r') as f:
            results = json.load(f)

        # Calculate quality metrics
        calculator = QualityMetricsCalculator()
        test_results = results.get('results', {}).get('table', [])

        # Convert to expected format
        formatted_results = []
        for test in test_results:
            formatted_results.append({
                'test_id': test.get('test_id', 'unknown'),
                'passed': test.get('success', False),
                'execution_time': test.get('latency', 0) / 1000,  # Convert ms to seconds
                'actual_output': test.get('output', ''),
                'expected_output': test.get('vars', {})
            })

        # Calculate composite score
        composite_score = calculator.calculate_composite_score(formatted_results)

        # Generate report
        report = calculator.generate_quality_report(composite_score)

        # Save report
        with open('quality_report.md', 'w') as f:
            f.write(report)

        print('Quality report generated')
        print(f'Composite Score: {composite_score[\"composite_score\"]:.3f}')
        "

    - name: Upload quality report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: quality-report
        path: quality_report.md

    - name: Check quality thresholds
      run: |
        python -c "
        import sys
        sys.path.append('src')
        from behavior.metrics_calculator import QualityMetricsCalculator
        import json

        # Load quality results
        with open('results.json', 'r') as f:
            results = json.load(f)

        calculator = QualityMetricsCalculator()
        test_results = results.get('results', {}).get('table', [])

        # Format results
        formatted_results = []
        for test in test_results:
            formatted_results.append({
                'test_id': test.get('test_id', 'unknown'),
                'passed': test.get('success', False),
                'execution_time': test.get('latency', 0) / 1000,
                'actual_output': test.get('output', ''),
                'expected_output': test.get('vars', {})
            })

        # Calculate metrics
        composite_score = calculator.calculate_composite_score(formatted_results)
        overall_score = composite_score.get('composite_score', 0.0)

        # Check against threshold (0.85 = 85%)
        threshold = 0.85
        if overall_score >= threshold:
            print(f'âœ… Quality threshold met: {overall_score:.3f} >= {threshold}')
        else:
            print(f'âŒ Quality threshold not met: {overall_score:.3f} < {threshold}')
            print('This may indicate issues with the system behavior.')

        # Check individual metrics
        individual_metrics = composite_score.get('individual_metrics', {})
        for metric_name, metric_value in individual_metrics.items():
            metric_threshold = calculator.thresholds.get(metric_name, 0.0)
            if metric_value < metric_threshold:
                print(f'âš ï¸  {metric_name} below threshold: {metric_value:.3f} < {metric_threshold}')
        "

    - name: Comment on PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = 'quality_report.md';

          if (fs.existsSync(path)) {
            const report = fs.readFileSync(path, 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## ğŸ¤– Promptfoo Quality Report\n\n${report}`
            });
          }